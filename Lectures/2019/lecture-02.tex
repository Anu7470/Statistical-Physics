% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[letterpaper,10pt,english]{article}
\input{../../header}
\title{Lecture-02: Mutual Information}
\author{}

\begin{document}
\maketitle

\section{Mutual Information}
\begin{defn} 
The \textbf{joint entropy} (in bits) of a pair of r.v. $(X, Y) \sim p_{X,Y}(x, y)$ is denoted
\EQ{
H(X,Y) \triangleq \sum_{(x,y) \in \sX \times \sY}p_{X,Y}(x, y)\log_2\frac{1}{p_{X,Y}(x,y)} = \E{\log_2\frac{1}{p_{X,Y}(X,Y)}}.
}
Notice that this is identical to $H(Z)$ with $Z = (X, Y)$.
\end{defn} 
\begin{defn}
For a pair of r.v. $(X, Y) \sim p_{X,Y}(x, y)$, the \textbf{conditional entropy} (in bits) of $Y$ given $X$ is denoted 
\EQ{
H(Y|X) \triangleq \sum_{(x,y) \in \sX \times \sY} p_{X,Y}(x,y)\frac{1}{\log_2 p_{Y|X}(y|x)} = \E{\frac{1}{\log_2 p_{Y|X}(Y|X)}}.
}
Notice that this equals entropy of the conditional distribution $p_{Y|X} (y|x)$ averaged over $x$.
\end{defn}

\begin{defn}
For a pair of r.v. $(X, Y ) \sim p_{X,Y}(x, y)$, the \textbf{mutual information} (in bits)
between $X$ and $Y$ is denoted
\EQ{
I(X;Y) \triangleq \sum_{(x,y) \in \sX \times \sY}p_{X,Y}(x,y)\log_2\frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)} = \E{\log_2\frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)}}.
}
\end{defn}
\begin{lem}
Basic properties of joint entropy and mutual information:
\begin{enumerate}
\item (chain rule of entropy) $H(X,Y) = H(X)+H(Y|X)$. If $X$ and $Y$ are independent,
$H(X, Y ) = H(X) + H(Y )$.
\begin{proof}
Take the expectation of $\log_2\frac{1}{p_{X,Y}(x,y)} =\log_2\frac{1}{p_X(x)} + \log_2\frac{1}{p_{Y|X}(y|x)}$ and note that $p_{Y |X}(y|x) = p_Y (y)$ for all $x, y$ if $X$ and $Y$ are independent.
\end{proof}
\item (mutual information) The mutual information satisfies 
\EQ{
I(X; Y) = H (X) + H (Y) - H (X, Y) = H (X) - H (X|Y) = H (Y) -H (Y|X).
}
\begin{proof}
Take the expectation of $\log_2\frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)} =\log_2\frac{1}{p_X(x)} + \log_2\frac{1}{p_Y(y)}  - \log_2\frac{1}{p_{X,Y}(x,y)}$ and apply the chain rule as needed. 
Also, symmetry follows from swapping $X,Y$ and $x, y$ in the sum because $p_{X,Y} (x, y) = p_{Y,X} (y, x)$. 
\end{proof}
\end{enumerate}
\end{lem} 
\begin{exmp}
Let $\sX = \sY = \set{0, 1}$ and $p_{X,Y}(x, y) = \frac{\rho}{2} \indicator{x \neq y} + \frac{(1 - \rho)}{2}\indicator{x=y}$. 
It follows that $p_X(x)=p_Y(y)= \frac{1}{2}$, and hence $H(X)= H(Y) = 1$. 
Since $p_{Y|X}(y|x) \in \set{\rho,1-\rho}$, it follows that $H(Y|X)= \cH(\rho)$. 
Thus, we have $I(X;Y)=H(Y) - H (Y |X) = 1 - \cH(\rho)$. 
The conditional distribution $p_{Y |X}$ called the \textbf{binary symmetric channel} with error probability $\rho$ and denoted by BSC($\rho$).
\end{exmp} 
 

\begin{defn}
The \textbf{Kullback-Liebler (KL) divergence} (in bits) between distributions $p(x)$ and $q(x)$, 
defined on the same support $\sX$, is denoted 
\EQ{
D(p\Vert q) \triangleq \sum_{x \in \sX}p(x)\log_2\frac{p(x)}{q(x)},
}
where we assume $0\log_2\frac{0}{q} = 0$ for $q \in [0,1]$ and $p\log_2\frac{p}{0} = \infty$ for $p > 0$. 
Thus, $D(p \Vert q) = \infty$ if there is any $x \in \sX$ such that $p(x)>0$ and $q(x)=0$.
\end{defn}  
\begin{rem}
The divergence is non-negative and equal to $0$ iff $p(x) = q(x)$ for all $x \in \sX$. 
Thus, it behaves something like a metric on the space of distributions. 
It is not exactly a metric, however, because it is not symmetric.
\end{rem}
\begin{exmp}
For $\sX = \set{0,1}$, let $p(1) = r$ define a Bernoulli($r$) distribution and $q(1) = s$ define a Bernoulli($s$) distribution. 
Then, the divergence between $p$ and $q$ is given by 
\EQ{
\cD(r \Vert s) \triangleq r\log_2\frac{r}{s} + (1-r)\log_2\frac{1-r}{1-s}. 
}
\end{exmp}
\begin{exmp} 
Let $X$ be the number of ones in a length-$n$ vector of \emph{i.i.d.} Bernoulli($s$) random variables. 
Then, the probability the resulting vector has exactly $rn$ ones is given by
\EQ{
\P(X=rn)= \binom{n}{rn} s^{rn}(1-s)^{n(1-r)}.
}
Using the results from the previous example, one can see that this equals
\EQ{
\frac{1 + O(\frac{1}{nr(1-r)})}{\sqrt{2\pi nr(1-r)}}2^{n\cH(r)}2^{n[r \log_2s+ (1-r)\log_2(1-s)]} = \frac{1 + O(\frac{1}{nr(1-r)})}{\sqrt{2\pi nr(1-r)}}2^{-n\cD(r\Vert s)}.
}
Thus, we see that exponential decay rate is determined by the divergence between a Bernoulli($r$) distribution and a Bernoulli($s$) distribution. 
This example highlights the connection between information theory and the theory of large deviations.
\end{exmp}

\begin{defn}
A function $f: \R \to \R$ is called \textbf{convex} on the interval $(a, b)$ if, for all $x_1, x_2 \in (a,b)$ and $\lambda \in [0,1]$,
\EQ{
f(\lambda x_1 + (1 -\lambda)x_2) \le \lambda f(x_1) + (1 -\lambda)f(x_2).
}
It is called \textbf{strictly convex} if equality holds only if $\lambda = 0$ or $\lambda = 1$. 
For a (strictly) convex function $f$, the function $-f$ is called (strictly) \textbf{concave}. 
\end{defn} 

\begin{lem}[Jensen's Inequality] 
If $f$ is convex and $X$ is a real random variable, then
\EQ{
\E{f(X)} \ge f(\E{X}).
}
If $f$ is strictly convex, the equality occurs iff $X = \E X$ with probability $1$. 
The inequality is simply reversed if $f$ is (strictly) concave.
\end{lem} 
\begin{proof} 
Since $f$ is convex, any tangent line to its graph must lower bound the function. 
Thus, for any $x_0 \in \R$, there is a constant $a \in \R$ such that the linear function $a(x - x_0) + f(x_0)$ lower bounds $f(x)$. 
If we choose $x_0 = \E X$, then it follows that
\EQ{
\E{f (X )} \ge \E{a(X - x_0 ) + f (x_0 )} = a\E X  - a\E X  + f(\E X ) = f (\E X).
}
If $f$ is strictly convex, then equality in the tangent lower bound occurs only at $x = x_0$. 
Thus, Jensen's inequality is strict unless $X = \E X$ with probability 1.
\end{proof}

\begin{thm}[Non-Negativity of Divergence]
\label{thm:NonNegDiv}
For $x \in \sX$, let $p(x)$ and $q(x)$ be two discrete distributions. 
Then, $D(p\Vert q) \ge 0$, with equality iff $p(x) = q(x)$ holds for all $x \in \sX$. 
This result holds even if $\sum_x q(x) < 1$.
\end{thm}
\begin{proof}
Let $A = \text{supp}(p) \triangleq \set{x \in \sX : p(x) > 0}$ be the support of $p(x)$. 
Then, 
\EQ{
-D(p\Vert q) %= -\sum_{x \in A}p(x)\log_2\frac{p(x)}{q(x)}
= \sum_{x \in A}p(x)\log_2\frac{q(x)}{p(x)} \le \log_2\sum_{x \in A}p(x)\frac{q(x)}{p(x)} \le \log_2\sum_{x \in \sX}q(x)= 0.
}
The first inequality holds with equality iff $p(x) = cq(x)$ for all $x \in A$. 
The second inequality holds iff $\sum_{x \in A} q(x)=1$. 
From these, we see that 
$c=1$ and $q(x)=p(x)=0$ for $x \in \sX\setminus A$.
\end{proof}
\begin{thm}[Convexity of Divergence] 
The divergence $D (p\Vert q)$ is convex in the pair $(p, q)$.
Thus, for two pairs, $(p_1, q_1)$ and $(p_2, q_2)$, we have
\EQ{
D(\lambda p_1 +(1-\lambda)p_2 \Vert \lambda q_1 +(1-\lambda)q_2) \le \lambda D(p_1 \Vert q_1)+(1-\lambda)D(p_2\Vert q_2)
}
for all $\lambda \in [0, 1]$.
\end{thm} 
\begin{proof} 
For non-negative numbers $a_1, a_2, \dots, a_n$ and $b_1, b_2, \dots, b_n$, 
we can form distributions $p = (\frac{a_1}{\sum_ia_i}, \dots, \frac{a_n}{\sum_ia_i})$ and $q = (\frac{b_1}{\sum_ib_i}, \dots, \frac{b_n}{\sum_ib_i})$ on the support $[n]$. 
From the non-negativity of Divergence, we get 
\EQ{
0 \le D(p \Vert q) = \sum_{i=1}^n\frac{a_i}{\sum_ia_i}\log_2\frac{a_i}{b_i} - \log_2\frac{\sum_{i=1}^na_i}{\sum_{i=1}^nb_i}, 
}
where the equality holds iff $\frac{a_i}{b_i}$ is constant for $i \in [n]$. 
This inequality is called the log-sum inequality
\EQ{
\sum_{i=1}^na_i\log_2\frac{a_i}{b_i} \ge (\sum_{i=1}^na_i)\log_2\frac{\sum_{i=1}^na_i}{\sum_{i=1}^nb_i},
}
where equality holds iff $\frac{a_i}{b_i}$ is constant for $i \in [n]$. 

One can apply this to the LHS of (1) to derive (1).
\end{proof}
\begin{lem}
More properties of entropy and mutual information:
\begin{enumerate}
\item $I (X; Y) \ge 0$ with equality iff $X$ and $Y$ are independent.
\begin{proof} 
First, we observe that $I(X;Y)=D(p_{X,Y}\Vert p_Xp_Y)$. 
By Theorem~\ref{thm:NonNegDiv}, this divergence is zero iff $p_{X,Y} (x, y) = p_X (x)p_Y (y)$ for all $x, y$, but this is precisely the definition of independence.
\end{proof} 
\item $H (Y |X) \le H (Y)$ with equality iff $X$ and $Y$ are independent.
\begin{proof} 
Since $H(Y)- H(Y|X) = I(X;Y) = D(p_{X,Y} \Vert p_Xp_Y) \ge 0$ iff $X$ and $Y$ are independent, this follow directly from the previous statement. 
\end{proof} 
\item The entropy $H(p)$ is concave in $p$ and the uniform distribution is the unique maximum. 
\begin{proof}
Given $p(x)$ defined on $\sX$, let $q(x) = 1/|\sX|$ for all $x \in \sX$. 
Then, we see that 
\EQ{
D(p\Vert q)= \sum_{x \in \sX}p(x)\log_2\frac{p(x)}{1/\abs{\sX}} = -H(p)+\log_2|\sX|. 
}
Solving for $H(p)$, we see that $H(p)$ is concave in $p$ because $D(p\Vert q)$ is convex in $p$. The uniform distribution gives the unique maximum because $D(p\Vert q) \ge 0$ with equality iff 
$p(x)$ is uniform.
\end{proof} 
\end{enumerate}
\end{lem} 
\begin{rem} 
From $I(X ; Y) = D (p_{X,Y} \Vert p_X p_Y )$, we see that the mutual information measures the difference between a joint distribution $p_{X,Y}$ and the product of its marginals $p_X p_Y$. 
\end{rem}
    
\section{Data Processing}
The definitions of entropy, mutual information, and divergence all extend naturally to any finite number of random variables by treating multiple random variables as a single random vector. However, there are a few new concepts that can only be defined in terms of three random variables. Let $X, Y$, and $Z$ be random variables with joint distribution $p_{X,Y,Z}(x,y,z)$. 
\begin{defn} 
For three r.v. $(X,Y,Z) \sim p_{X,Y,Z}(x,y,z)$ defined on $\sX \times \sY \times \sZ$, 
the conditional mutual information (in bits) between $X$ and $Y$ given $Z$ is denoted 
\EQ{
I(X;Y\given Z)  \triangleq \sum_{(x,y,z) \in \sX \times \sY \times \sZ}p_{X,Y,Z}(x,y,z)\log_2\frac{p_{X,Y|Z}(x,y,z)}{p_{X|Z}(x,z)p_{Y|Z}(y,z)} = \E{\log_2\frac{p_{X,Y|Z}(X,Y,Z)}{p_{X|Z}(X,Z)p_{Y|Z}(Y,Z)}}.
}
From this, we see that $I(X;Y\given Z) = H(X | Z) + H(Y|Z) - H(X,Y |Z)$. 
Thus the conditioning is simply inherited by each entropy in the standard decomposition. 
\end{defn} 
\begin{defn} 
Three r.v. $(X, Y, Z) \sim p_{X,Y, Z} (x, y, z)$ form a Markov chain $X -Y - Z$ if 
\EQ{
p_{X,Y,Z} (x, y, z) = p_X (x)p_{Y |X} (y|x)p_{Z|Y}(z|y).
}
This is clearly the same as $p_{Z|X,Y} (z|x, y) = p_{Z|Y} (z | y)$ for all $x, y, z$, 
which is equivalent to the condition that $X$ and $Z$ are conditionally independent given $Y$. 
\end{defn} 

\begin{lem}
Properties of mutual information for three random variables:
\begin{enumerate}
\item (chain rule of mutual information) $I(X;Y,Z)=I(X;Y)+I(X;Z|Y)$. 
\begin{proof}
This follows from the expectation of the decomposition
\EQ{
\log_2\frac{p_{X,Y,Z}(X,Y,Z)}{p_X(x)p_{Y,Z}(Y,Z)}= \log_2\frac{p_{X,Y}(X,Y)p_{Z|X,Y}(Z|X,Y)}{p_X(x)p_Y(Y)p_{Z|Y}(Z|Y)}
= \log_2\frac{p_{X,Y}(X,Y)}{p_X(x)p_Y(Y)} + \log_2\frac{p_{X,Z |Y}(X,Z|Y)}{p_{Z|Y}(Z|Y)p_{X|Y}(X|Y)}.
}
\end{proof} 
\item (non-negativity of conditional mutual information) $I (X ; Y |Z ) \ge 0$ with equality iff $X$ and $Y$ are conditionally independent given $Z$.
\begin{proof}
First, we observe that
\EQ{
I (X; Y |Z) = \sum_z p_Z(z)D(p_{X,Y |Z=z}\Vert p_{X|Z=z}p_{Y |Z=z}).
} 
Each term in this sum is non-negative and equal to zero iff $p_{X,Y|Z=z}(x,y) = p_{X|Z=z}(x)p_{Y|Z=z}(y)$ for all $x,y$. 
Thus, the overall sum is zero iff the condition holds for all $x, y, z$ (i.e., $X$ and $Y$ are conditionally independent given $Z$). 
\end{proof} 
\end{enumerate}
\end{lem} 
    
\begin{thm}[Data Processing Inequality] 
If three r.v. $(X, Y, Z) \sim p_{X,Y,Z} (x, y, z)$ form a Markov chain $X - Y- Z$, then 
$I(X;Z) \le I(X;Y)$. 
For example, if $Z=f(Y)$ is a function of $Y$, 
then $X -Y - Z$ form a Markov chain. 
\end{thm} 
\begin{proof}
Applying the chain rule of mutual information in the two possible orders gives 
\EQ{
I (X; Y, Z) = I (X; Z) + I (X; Y |Z) = I (X; Y) + I (X; Z|Y).
}
Since $X - Y - Z$ form a Markov chain, $X$ and $Z$ are conditionally independent and 
$I(X;Z|Y) = 0$. 
Thus, we have 
\EQ{
I (X; Y ) = I (X; Z) + I (X; Y |Z) \ge I (X; Z).
}
If $Z = f(Y)$, then $p_{Z|X,Y}(z|x,y) = \indicator{z =f(y)} = p_{Z|Y}(z,y)$ and $X - Y - Z$ form a Markov chain.
\end{proof}. 
\begin{exmp}
A system has a random state $X$ and an experiment with outcome $Y$ is performed to measure that state. 
Is it possible that additional processing can produce a new output $Z = f(Y)$ such that 
$H (X|Z) < H (X|Y )$?
\end{exmp}
\begin{thm} [Fano's Inequality] 
Let the r.v. $Y$ be an observation of the r.v. $X$ and $\hat{X} = f(Y)$ be an estimate of $X$. 
Then, the error probability $P_e = P(\hat{X} \neq X)$ satisfies 
\EQ{
\cH(P_e) + P_e \log_2 (|\sX| - 1) \ge H (X|Y).
}
\end{thm} 
\begin{proof}
Let $E = \indicator{\hat{X}= X}$ be an indicator r.v. for the error event. 
Expanding the conditional entropy $H(E, X |\hat{X})$ in two ways gives 
\EQ{
H(E, X |\hat{X}) = H(X| \hat{X}) + H(E|X, \hat{X}) = H(E| \hat{X}) + H(X|E, \hat{X}).
}
Now $H(X|\hat{X}) \ge H(X|Y)$ by data processing inequality, since $X - Y - \hat{X}$ form a Markov chain, 
and $H(E| X, \hat{X}) = 0$ since $E = \indicator{X \neq \hat{X}}$. 
Further, $H(E|\hat{X}) \le H(E) = \cH(P_e)$ since the conditioning reduces entropy. 
In addition, $H(X|E=0, \hat{X}) = 0$, and we can write 
\EQ{
H(X|E=1, \hat{X}) \le H(X \neq \hat{X}) \le \log_2(\abs{\sX}-1). 
}
This implies that $H(X|E, \hat{X}) \le P_e\log_2(\abs{\sX}-1)$. 
Rearranging these terms gives the stated result.
\end{proof}
\section{Sequences of random variables}
Let $(X_t: t \in \N)$ be a random process where each random variable lies in $\sX$. 
The joint probability distribution of the first $N$ random variables is denoted 
$P_N (x_1, \dots , x_N)$. 
Let $[N] \triangleq \set{1,2, \dots,N}, A \subseteq [N]$, and $\bar{A} = [N]\setminus A$ be sets of indices. 
We will denote subvectors with indices in $A$ and $\bar{A}$ by
\meq{2}{
&x_A =(x_t: t\in A),&&x_{\bar{A}} =(x_t: t \in \bar{A}).
}
The marginal distribution of variables in $A$ is given by summing over all variables in $\bar{A}$: \EQ{
P_A(x_A) = \sum_{x_{\bar{A}}}P_N(x_1, \dots,x_N).
}
\begin{defn}
The entropy rate of a random process is defined to be
\EQ{
h_X \triangleq \lim_{N \to \infty} \frac{1}{N}H(X_1,X_2, \dots,X_N),
}
if the limit exists.
\end{defn} 
\begin{exmp}
If the random variables are drawn i.i.d. according to $p(x)$, then 
\EQ{
P_N(x_1, \dots, x_N) = \prod_{t=1}^Np(x_t). 
}
In this case, $H (X_1, \dots,X_N) = NH (p)$ and the entropy rate is $h_X = H (p)$.
\end{exmp} 
\begin{exmp}
If the random variables form a homogenous Markov chain, then 
\EQ{
P_N(x_1, \dots ,x_N) = p_1(x_1)\prod_{t=1}^N w(x_t \to  x_{t+1}),
}
where $p_1(x)$ is the distribution of the initial state and $w(x \to x') = p_{X_{t+1}|X_t} (x'|x)$ defines the transition probabilities of the chain. 
In this case, the entropy rate is given by
\eq{
h_X &= \lim_{N \to \infty} \frac{1}{N}H(X_1,X_2, \dots,X_N) = \lim_{N \to \infty} \frac{1}{N}\left(H(X_1) + \sum_{t=1}^{N-1}H(X_{t+1}|X_t)\right)\\
&= \lim_{N \to \infty} \frac{1}{N}\sum_{t=1}^{N-1}\sum_{x \in \sX}p_t(x)\sum_{x' \in \sX}w(x \to x')\log_2\frac{1}{w(x \to x')}\\
&=\sum_{x \in \sX}\left(\lim_{N \to \infty} \frac{1}{N} \sum_{t=1}^{N-1}p_t(x)\right)\sum_{x' \in \sX}w(x \to x')\log_2\frac{1}{w(x \to x')}\\
&=\sum_{x \in \sX}p^{\ast}(x)\sum_{x' \in \sX}w(x \to x')\log_2\frac{1}{w(x \to x')},
}
where the last step assumes that $w(x \to x')$ was chosen so that the limiting occupancy
distribution $p^{\ast}(x) = \lim_{N \to \infty} \sum_{t=1}^{N-1}p_t(x)$ exists and is independent of the initial state distribution. 
\end{exmp}
\end{document}
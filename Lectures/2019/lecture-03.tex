% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[letterpaper,10pt,english]{article}
\input{../../header}
\title{Lecture-01: Introduction to Information Theory}
\author{}

\begin{document}
\maketitle

\section{ Data Compression}
Consider a source that generates a sequence of symbols taking values in a finite alphabet $\sX$.  Such a source is typically modeled by a random process $(X_t: t \in \N)$. 
For the purpose of communication and storage, 
it is desirable to encode this sequence using as few bits as possible. 
If exact reconstruction is required, then this is known as \textbf{lossless source coding}.  

The most natural approach to this problem is to encode length-$N$ source blocks 
$(X_1, \dots , X_N)$ into variable-length blocks of bits. 
Let $\set{0, 1}^{\ast} = \cup_{n=0}^{\infty}\set{0, 1}^n$ denote the set of finite-length binary strings. Then, the source encoder is a function 
\eq{
w:\sX^N &\to \set{0,1}^{\ast}\\
 x &\mapsto w(x). 
}
If the source sequence consists of the length-N blocks x(1),x(2),...,x(r), then the encoded sequence is the concatenation w(x(1))w(x(2)) . . . w(x(r)). Since the output sequence does not add markers between blocks, one must choose the w carefully to guarantee decodability. The standard approach is use to prefix-free (or instantaneous) codes where no codeword is a prefix of another codeword. This allows the decoder to uniquely reconstruct the codeword boundaries.
An important property of a code is its average length. If lw(x) is the length of w(x) in bits, then the average length of an encoded block is
L(w) = ?? PN (x)lw(x) bits. x?X N
Any prefix-free code can be represented by a binary tree whose leaf nodes are labeled by codewords. To construct a prefix-free code, one can draw a binary tree and sequentially assign x values to nodes. After each assignment, all children of the assigned node are removed.
Exercise 43. Is there a prefix-free code with codeword lengths 1, 2, 3, 3? How about 2, 2, 3, 3, 3, 4, 4, 4? Try constructing a code for each case.
\begin{lem}\end{lem} 44 (Kraft Inequality). A prefix-free source code with length function lw(x) exists iff
?? 2?lw(x) ? 1. x?X N
13
                 
\begin{proof}\end{proof}. Let lmax = maxx?XN lw(x) and recall that a binary tree has exactly 2l nodes at depth- l. To construct a code, one starts with the complete binary tree of depth lmax. Then, for each x ? supp(PN) (in order of increasing length), one assigns x to a codeword w(x) of length lw(x). For an x with length lw(x), one finds an available node at depth lw(x), assigns the binary label of that node to w(x), and then removes all children of that node. Assigning a codeword of length lw(x) removes exactly 2l?lw(x) nodes at depth-l for l ? lw(x). Thus, this process succeeds up to depth-l if and only if
??
2l?lw(x) ? 2l. x:lw (x)?l
Dividing by 2l, one see that this condition is most restrictive for l = lmax. Choosing l = lmax and dividing by 2lmax gives the desired result.
\begin{thm}\end{thm} 45 (Source Coding \begin{thm}\end{thm}). For the distribution PN(x), let L?N be average length of an encoded block for the prefix-free code with the minimum average length. Then,
H(X)?L?N ?H(X)+1.
\begin{proof}\end{proof}. Let lw(x) be the length function for a valid prefix-free code and define QN(x) = 2?lw(x). Since lw(x) must satisfy the Kraft inequality, it follows that ??x QN(x) ? 1. Using \begin{thm}\end{thm} 30 (which holds even if ??x q(x) ? 1), we see that the average code length, L, satisfies
14
                          L?H(PN)= =
????1??
PN(x) lw(x)?log2 PN(x) ?? ?? PN(x)??
   x?X
  PN(x) log2 QN(x)
= D(PN?QN) (2)
   x?X
  ? 0.
If we choose lw(x) to be the length function for a code that achieves the optimal L = L?N, then this implies that L?N ? H (PN ). To achieve the upper bound, we design a code with
 
lw (x) = ?? log2 PN (x)? and compute
15
  L?N ? ??PN(x)??log2 PN(x)? x?X
   ????1??
?
Together, these complete the proof.
\begin{rem}\end{rem} 46. This shows that one operational definition of the entropy is ?the minimum
average length of any variable length code that can be used to reconstruct X?.
\begin{rem}\end{rem} 47. In theory, one can use \begin{thm}\end{thm} 45 to achieve optimal compression rate for i.i.d. sequences. To see this, we observe that H(PN) = NH(X1). Thus, by increasing N, the constructive upper bound in the theorem gives a compression rate (i.e., bits per source symbol) of
L ?N ? H ( X 1 ) + 1 . NN
\begin{exmp}\end{exmp} 48. Let us consider what happens if a code is designed for a different distribution, QN , and then used with the distribution PN . From (2), we see that the average code length, L, must satisfy L ? H (PN ) + D (PN ?QN ). On the other hand, if the lengths are chosen to be lw (x) = ?? log2 QN (x)?, then the average length satisfies
L = ??PN(x)log2 ??log2 QN(x)? x?X
????1??
\begin{rem}\end{rem} 49. This shows that one operational definition of the divergence D (PN ?QN ) is ?the increaseinaveragelengthassociatedwithdesigningacodeforQN whenthetruedistribution is PN ?.
PN(x) log2 PN(x)+1 = H (X) + 1.
  x?X
            PN(x) log2 QN(x)+1
?? ?? PN(x)??
PN(x) log2 QN(x) = H (X) + 1 + D (PN ?QN ) .
= =H(X)+1+
  x?X
      x?X
   
\section{Communication Channels}
In engineering, one often wants to communicate information across an unreliable medium. For example, think of a system that modulates the current in a wire (by adjusting the voltage at one end) and measures the current at the other end. Due to thermal fluctuations, the difference between the modulated current at the measured current will always contain some randomness. One can analyze this situation by first discretizing time and then defining a simple mathematical model.
\begin{defn}\end{defn} 50. A discrete memoryless channel (DMC) is defined by a finite input alphabet X, a finite output alphabet Y, and a conditional probability distribution Q(y|x). For N ? N channel uses, let the channel input vector be a random vector X = (X1,...XN) ? XN. Then, the channel output vector is a random vector Y = (Y1,...,YN) ? YN where
N
QN(y|x) ?? P??Y = y????X = x?? = ??Q(yi|xi).
t=1
\begin{exmp}\end{exmp} 51. For example, the binary symmetric channel (BSC) with error probability
? has X = Y = {0, 1} and is defined by
Q(y|x) = (1 ? ?)I(x = y) + ?I(x?= y).
\begin{exmp}\end{exmp} 52. For example, the binary erasure channel (BEC) with erasure probability ? has X = {0,1}, Y = {0,1,?}, and is defined by
Q(y|x) = ?I(y = ?) + (1 ? ?)I(y = x).
Channel coding is the process of improving performance by adding redundancy (e.g.,
by encoding an K bit message into N > K bits).
\begin{defn}\end{defn} 53. For binary-input channel, a length-N code carrying an K-bit message is defined by an encoder that maps m ? {0,1}K to a codeword x(m) ? {0,1}N. The ratio R = K/N is called the rate (in information bits per channel use) of the code . A message decoder xd : YN ? {0,1}K is a mapping from the channel output to one of the possible input messages.
\begin{exmp}\end{exmp} 54. For a BSC, the simplest approach is to simply repeat each bit N times and decode via majority vote (i.e., x(0) = 00...00, x(1) = 11...11, and xd(y) =
16
              
????N ??
I i=1 yi > N/2 ). In this case, the original bit will be recovered correctly as long as there
are no more than ?N/2? errors. Thus, one can achieve arbitrary reliability by increasing N. But, increasing N also reduces the rate of communication.
\begin{defn}\end{defn} 55. For a code/decoder pair, the block error probability of message m is the probability,
PB(m) = ?? QN ??y|x(m)?? I ??xd(y)?= m?? , y?YN
that decoder does not return m when message m is transmitted. The maximum and average block error probabilities are denoted
PBmax ? max PB(m) m?{0,1}K
17
     PBav ? 1 ?? PB(m).
 ??
C ?maxI(X;Y)=max
p(x) p(x) (x,y)?X×Y
p(x)Q(y|x)log2 ??
Q(y|x)
? ? .
2M
m?{0,1}K
\begin{defn}\end{defn} 56. A code rate R is achievable if there exists a sequence of encoder/decoder pairs with rate RN ? R and block error rate PB,N ? 0. The channel capacity is the supremum of all achievable code rates.
\begin{rem}\end{rem} 57. One can get a qualitative feel for achievable rates via the following argument. The key is that, for i.i.d. sequences (X1 , . . . , XN ) with large N , the probability distribution essentially becomes uniform over a set of 2NH(X) ?typical? sequences. Thus, for (X,Y) ? p(x)Q(y|x), the i.i.d. sequence ((X1, Y1), . . . , (XN , YN )) takes one of 2NH(X,Y ) different typical values with essentially uniform probability. If we ignore the X values, then the number of (Y1,...,YN) typical sequences is roughly 2NH(Y). If we fix the (X1,...,XN) sequence to a typical value (x1,...,xN), then the number of ((x1,Y1),...,(xN,YN)) typical sequences is roughly 2NH(Y |X). This last set of sequences can be seen as the likely set of output sequences if x is transmitted. Thus, if the likely output sets of each codeword fill the space but do not overlap,thenweget2NR2NH(Y|X) =2NH(Y) orR=H(Y)?H(Y|X)=I(X;Y).
\begin{thm}\end{thm} 58 (Channel Coding \begin{thm}\end{thm}). For a DMC, the channel capacity is given by
  Thus, for any R ? C, there exists a sequence of encoder/decoder pairs such that RN ? R and PB,N ? 0. Conversely, if a sequence of encoder/decoder pairs satisfies RN ? R and PB,N ?0,thenR?C.
x? p(x)Q(y|x)

\begin{proof}\end{proof}. Achievability will be shown in the lecture devoted to Chapter 6. The following converse demonstrates the power and simplicity of information theory.
For any length-N encoder/decoder pair, let M be a uniform random message, X = x(M) be its encoded codeword, Y be the channel output, and M? = xd(Y ) be the decoded message. Since M ?X ?Y ?M? form a Markov chain, we have
NR=H(M) (H(M)=K =NR) ???????????? ?????
18
       =H M????M +I M;M
?H(PB)+PBlog2??2NR ?1??+I??M;M???
? 1 + P B N R + I ?? M ; M? ??
? 1 + PBNR + I (X; Y ) ? 1 + PB N R + N C
Solving for an upper bound on R, we find that 1??1??
(Def.ofI M;M ) (Fano?sInequality)
(Data Processing Inequality) (See \begin{lem}\end{lem} 60).
  R?1?P C+N . B
  Thus,RN ?R?CforanysequencewhereN??andPB,N ?0.
Exercise 59. Verify that the capacity of the BEC(?) channel is C = 1 ? ? and the capacity
oftheBSC(?)channelisC=1?H(?). Hint: UseI(X;Y)=H(Y)?H(Y|X). \begin{lem}\end{lem} 60. For N channel uses on a DMC,
 I (X; Y ) ? NC.
  \begin{proof}\end{proof}. This follows from
I (X; Y ) = H (Y ) ? H (Y |X) NN
= ?? H (Yi|Y1, . . . , Yi?1) ? ?? H (Yi|Y1, . . . , Yi?1, X) i=1 i=1
NN =??H(Yi|Y1,...,Yi?1)???H(Yi|Xi)
i=1 i=1 NN
? ?? H (Yi) ? ?? H (Yi|Xi) i=1 i=1
N ???I(Xi;Yi)?NC
i=1
(Chain Rule of Entropy) (Yi cond. ind. givenXi) (conditioning reduces entropy) (C maximizesI(X;Y)).
       
 
\end{document}